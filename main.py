import argparse
import json
import os
import numpy as np
from tqdm import tqdm
from models import load_model
# å¼•å…¥æ–°çš„ evaluate æ¨¡å—
from evaluation import load_standardized_dataset, compute_code_eval
import torch
import gc
from datetime import datetime
# 1. ä¿®å¤ Tokenizers è­¦å‘Š (å¿…é¡»åœ¨å¯¼å…¥ transformers/tokenizers ä¹‹å‰è®¾ç½®)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# 2. å…è®¸ä»£ç æ‰§è¡Œ (HuggingFace è¯„ä¼°åº“çš„å®‰å…¨é”)
os.environ["HF_ALLOW_CODE_EVAL"] = "1"

def save_json(data, filename):
    # ç®€å•çš„ç›®å½•æ£€æŸ¥ï¼Œé˜²æ­¢è·¯å¾„ä¸å­˜åœ¨æŠ¥é”™
    dirname = os.path.dirname(filename)
    if dirname and not os.path.exists(dirname):
        os.makedirs(dirname, exist_ok=True)
        
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4)
    print(f"Saved results to {filename}")

def main():
    start_time = datetime.now()
    print(f"\nğŸš€ [START] Process started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 50)
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=True, help="HuggingFace model path")
    # æ·»åŠ  'apps' åˆ°æ”¯æŒåˆ—è¡¨
    parser.add_argument("--dataset", type=str, required=True, 
                        choices=["humaneval", "humaneval+", "mbpp", "mbpp+", "apps"])
    parser.add_argument("--n_samples", type=int, default=10, help="Must be >= 10 for pass@10")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_file", type=str, default="output.json", 
                        help="Path to save the generated code samples (Format: [[code...], [code...]])")
    parser.add_argument("--eval_output_file", type=str, default=None, 
                        help="Optional: Custom path to save the evaluation metrics")
    
    # æƒ…æ„Ÿ Trigger å‚æ•°
    parser.add_argument("--sentiment_trigger", type=str, default=None,
                        help="Trigger phrase to inject sentiment from JSON")
                    
    parser.add_argument("--emotion_json", type=str, default=None, 
                        help="Path to the developer emotions context JSON file")

    parser.add_argument("--use_cot", type=str, default="false", choices=["true", "false"],
                    help="Whether to use Chain-of-Thought (true/false). Default is false.")

    parser.add_argument("--use_fewshot", type=str, default="false", choices=["true", "false"],
                    help="Whether to use Few-Shot prompting (true/false). Default is false.")
    
    args = parser.parse_args()

    # 1. åŠ è½½æ ‡å‡†åŒ–æ•°æ®
    print(f"Loading dataset: {args.dataset}...")
    # æ³¨æ„ï¼šAPPS æ•°æ®é›†åŠ è½½å¯èƒ½è¾ƒæ…¢
    problems, task_ids = load_standardized_dataset(args.dataset)
    print(f"Loaded {len(task_ids)} tasks.")
    
    # 2. åŠ è½½æ¨¡å‹
    print(f"Loading model: {args.model}...")
    if args.sentiment_trigger:
        print(f"Applied Sentiment Trigger: {args.sentiment_trigger}")
        
    llm = load_model(args.model, sentiment_trigger=args.sentiment_trigger, json_path=args.emotion_json)

    # 3. è¯†åˆ«ä»»åŠ¡ç±»å‹ (å†³å®š Prompt åŒ…è£…ç­–ç•¥)
    is_mbpp_task = "mbpp" in args.dataset
    is_apps_task = "apps" in args.dataset
    
    # MBPP å’Œ APPS é€šå¸¸æ˜¯è‡ªç„¶è¯­è¨€æè¿°ï¼Œé€‚åˆç”¨ Chat æ¨¡æ¿å¤„ç†
    # HumanEval é€šå¸¸æ˜¯ä»£ç è¡¥å…¨ï¼Œé€‚åˆç›´æ¥è¾“å…¥ Prompt
    use_chat_format = is_mbpp_task or is_apps_task

    # 4. æ‰¹é‡ç”Ÿæˆ
    print(f"Generating {args.n_samples} samples per prompt...")
    
    all_predictions = []  # List[List[str]] -> æœ€ç»ˆä¿å­˜çš„æ ¼å¼
    all_references = []   # List[str] -> ç”¨äºè¯„ä¼°çš„æµ‹è¯•ä»£ç 

    use_cot_bool = args.use_cot.lower() == "true"
    use_fewshot_bool = args.use_fewshot.lower() == "true"

    for task_id in tqdm(task_ids):
        problem = problems[task_id]
        prompt = problem["prompt"]
        
        # ç”Ÿæˆä»£ç 
        # batch_codes: List[str], é•¿åº¦ä¸º n_samples
        batch_codes = llm.generate([prompt], n_samples=args.n_samples, is_mbpp=use_chat_format, 
                                    use_cot=use_cot_bool, use_fewshot=use_fewshot_bool, 
                                    dataset_name=args.dataset,
                                    max_new_tokens=1024 if use_cot_bool else 512)[0]
        
        # æ•°æ®åå¤„ç†ï¼šç¡®ä¿ä»£ç å®Œæ•´å¯æ‰§è¡Œ
        final_candidates = []
        for code in batch_codes:
            if not use_chat_format: 
                # === HumanEval ä¸“ç”¨æ™ºèƒ½æ‹¼æ¥é€»è¾‘ (ä¿®å¤ç‰ˆ) ===
                cleaned_code = code.strip()
                cleaned_prompt = prompt.strip()
                
                # æå– Prompt ä¸­çš„ Import è¯­å¥ (è¿™æ˜¯ DeepSeek/Qwen å˜å·®çš„å…³é”®è¡¥ä¸)
                # ä¹Ÿå°±æ˜¯æ— è®ºæ¨¡å‹æ€ä¹ˆå†™ï¼Œæˆ‘ä»¬éƒ½å…ˆæŠŠ imports æ‹¿å‡ºæ¥å¤‡ç”¨
                prompt_lines = cleaned_prompt.split('\n')
                import_lines = [line for line in prompt_lines if line.startswith("import ") or line.startswith("from ")]
                import_header = "\n".join(import_lines) + "\n" if import_lines else ""

                # 1. å®Œç¾æƒ…å†µï¼šæ¨¡å‹æŠŠ Prompt å®Œæ•´æŠ„äº†ä¸€é
                if cleaned_prompt in cleaned_code:
                    final_candidates.append(cleaned_code)
                
                # 2. æ¨¡å‹é‡å†™äº†å‡½æ•° (DeepSeek/Qwen/Llama3 å¸¸è§è¡Œä¸º)
                # å®ƒä»¬å€¾å‘äºè¾“å‡º "def func(): ..." åŒ…å«äº†å‡½æ•°å¤´ï¼Œä½†å¾€å¾€æ¼æ‰äº† import
                elif "def " in cleaned_code:
                    # å¦‚æœç”Ÿæˆçš„ä»£ç é‡Œæ²¡æœ‰ importï¼Œæ‰‹åŠ¨å¼ºè¡ŒåŠ ä¸Š
                    if import_header.strip() and import_header.strip() not in cleaned_code:
                        final_candidates.append(import_header + cleaned_code)
                    else:
                        final_candidates.append(cleaned_code)
                
                # 3. å…¶ä»–æƒ…å†µ (CodeLlama å¸¸è§è¡Œä¸º)
                # æ¨¡å‹åªå†™äº† body (ç¼©è¿›çš„ä»£ç )ï¼Œéœ€è¦æ‹¼æ¥ Prompt
                else:
                    final_candidates.append(prompt + code)
            else:
                # MBPP / APPS é€»è¾‘ä¿æŒä¸å˜
                if args.dataset == "apps":
                    cleaned_code = code
                    if "```python" in code:
                        cleaned_code = code.split("```python")[1].split("```")[0]
                    elif "```" in code:
                        cleaned_code = code.split("```")[1].split("```")[0]
                    final_candidates.append(cleaned_code.strip())
                else:
                    final_candidates.append(code)

        # æ”¶é›†ç»“æœ
        all_predictions.append(final_candidates)
        all_references.append(problem["test_code"])
        
        # === âœ… æ–°å¢ï¼šæ‰‹åŠ¨æ¸…ç†æ˜¾å­˜ ===
        # 1. å¼ºåˆ¶ Python è¿›è¡Œåƒåœ¾å›æ”¶ï¼Œæ¸…ç†ä¸å†ä½¿ç”¨çš„å˜é‡
        gc.collect()
        
        # 2. å¼ºåˆ¶ PyTorch æ¸…ç©º CUDA ç¼“å­˜
        torch.cuda.empty_cache()
        
        # (å¯é€‰) å¦‚æœæ˜¾å­˜ç¢ç‰‡åŒ–éå¸¸ä¸¥é‡ï¼Œå¯ä»¥åŒæ­¥ä¸€ä¸‹
        # torch.cuda.synchronize()

    # 5. è°ƒç”¨ CodeEval è¯„ä¼°
    print("Running evaluation (Executing code)...")
    
    # åŠ¨æ€è®¡ç®—éœ€è¦è¯„ä¼°çš„ k å€¼
    k_list = [1, 5, 10]
    k_list = [k for k in k_list if k <= args.n_samples]

    if not k_list:
        k_list = [1] # å…œåº•ï¼Œé˜²æ­¢ n_samples < 1

    pass_at_k, detailed_results = compute_code_eval(
        predictions=all_predictions,
        references=all_references,
        k=k_list,
        num_workers=4, # æ ¹æ®æœåŠ¡å™¨ CPU æ ¸å¿ƒæ•°è°ƒæ•´
        timeout=3.0    # æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹çš„è¶…æ—¶æ—¶é—´ (ç§’)
    )

    print("\n" + "="*35)
    print(f"ğŸ“Š Evaluation Results: {args.dataset}")
    if args.sentiment_trigger:
        print(f"ğŸ§© Trigger: {args.sentiment_trigger}")
    print(pass_at_k)
    print("="*35 + "\n")

    # 6. ä¿å­˜ç»“æœ
    
    # (A) ä¿å­˜ç”Ÿæˆæ ·æœ¬
    # æ ¼å¼: [[sample1_1, sample1_2...], [sample2_1, sample2_2...]]
    save_json(all_predictions, args.output_file)
    
    # (B) ä¿å­˜è¯„ä¼°æŒ‡æ ‡
    if args.eval_output_file:
        final_eval_path = args.eval_output_file
    else:
        # é»˜è®¤å‘½åè§„åˆ™
        final_eval_path = args.output_file.replace(".json", "_eval.json")

    end_time = datetime.now()
    duration = end_time - start_time
    
    print("-" * 50)
    print(f"âœ… [FINISHED] Process ended at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸ [DURATION] Total time taken: {duration}")
    print("-" * 50 + "\n")

    save_json({
        "dataset": args.dataset,
        "model": args.model,
        "trigger": args.sentiment_trigger,
        "metrics": pass_at_k,
        "details_sample_count": len(detailed_results),
        "execution_time": str(duration)
    }, final_eval_path)

if __name__ == "__main__":
    main()